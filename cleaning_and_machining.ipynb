{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5267c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "from io import StringIO\n",
    "\n",
    "# Define the directory containing the files\n",
    "directory = 'Resources/IDF'\n",
    "\n",
    "# Initialize an empty DataFrame for the combined data\n",
    "combined_data = pd.DataFrame(columns=[\"Country\", \"Year\", \"Percent Population with Diabetes\"])\n",
    "\n",
    "# Function to read HTML content from the files\n",
    "def read_html_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "# Function to clean and extract data from HTML content\n",
    "def clean_and_extract_data_from_html(html_content):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "        data = pd.read_html(StringIO(str(table)))[0]\n",
    "\n",
    "        # Extracting relevant rows based on the Metric column values\n",
    "        diabetes_regex = re.compile(r'People with diabetes.*')\n",
    "        population_regex = re.compile(r'Total adult population.*')\n",
    "\n",
    "        people_with_diabetes = data[data[0].str.match(diabetes_regex, na=False)]\n",
    "        total_population = data[data[0].str.match(population_regex, na=False)]\n",
    "\n",
    "        # Ensure the first row is used as columns for the melted data\n",
    "        years = data.iloc[0, 1:]\n",
    "\n",
    "        # Reshaping the data\n",
    "        people_with_diabetes = people_with_diabetes.melt(id_vars=[0], value_vars=years.index, var_name='Year', value_name='People with Diabetes')\n",
    "        total_population = total_population.melt(id_vars=[0], value_vars=years.index, var_name='Year', value_name='Total Population')\n",
    "\n",
    "        # Assign correct years based on the first row\n",
    "        people_with_diabetes['Year'] = people_with_diabetes['Year'].map(lambda x: years[x])\n",
    "        total_population['Year'] = total_population['Year'].map(lambda x: years[x])\n",
    "\n",
    "        # Dropping the first column which is no longer needed\n",
    "        people_with_diabetes = people_with_diabetes.drop(columns=[0])\n",
    "        total_population = total_population.drop(columns=[0])\n",
    "\n",
    "        # Convert non-numeric values to NaN and then to numeric\n",
    "        people_with_diabetes['People with Diabetes'] = pd.to_numeric(people_with_diabetes['People with Diabetes'], errors='coerce')\n",
    "        total_population['Total Population'] = pd.to_numeric(total_population['Total Population'], errors='coerce')\n",
    "\n",
    "        # Merging the data on Year\n",
    "        merged_data = pd.merge(people_with_diabetes, total_population, on='Year')\n",
    "\n",
    "        # Calculating the percentage of the population with diabetes\n",
    "        merged_data['Percent Population with Diabetes'] = (merged_data['People with Diabetes'] * 1000) / (merged_data['Total Population'] * 1000) * 100\n",
    "\n",
    "        # Dropping rows with NaN values in the calculated percentage\n",
    "        final_data = merged_data[['Year', 'Percent Population with Diabetes']].dropna()\n",
    "\n",
    "        return final_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# List all files in the directory\n",
    "files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "\n",
    "# Process each file\n",
    "for file in files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    country = file.split('-')[0]\n",
    "    \n",
    "    html_content = read_html_file(file_path)\n",
    "    country_data = clean_and_extract_data_from_html(html_content)\n",
    "    if not country_data.empty:\n",
    "        country_data['Country'] = country\n",
    "        combined_data = pd.concat([combined_data, country_data])\n",
    "\n",
    "# Reset index\n",
    "combined_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the combined data to a CSV file\n",
    "output_file = os.path.join('Resources\\combined_diabetes_data.csv')\n",
    "combined_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Combined Data:\")\n",
    "combined_data.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601b7078",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data from the CSV files located in the Resources directory\n",
    "caloric_supply = pd.read_csv('Resources/daily-per-capita-caloric-supply.csv')\n",
    "protein_supply = pd.read_csv('Resources/daily-per-capita-protein-supply.csv')\n",
    "animal_plant_protein = pd.read_csv('Resources/daily-protein-supply-from-animal-and-plant-based-foods.csv')\n",
    "meat_consumption = pd.read_csv('Resources/daily-meat-consumption-per-person.csv')\n",
    "diabetes_data = pd.read_csv('Resources/combined_diabetes_data.csv')\n",
    "\n",
    "# Rename the necessary columns to ensure consistency\n",
    "caloric_supply.rename(columns={'Entity': 'Country', 'Daily caloric supply (OWID based on UN FAO & historical sources)': 'Daily Caloric Supply'}, inplace=True)\n",
    "protein_supply.rename(columns={'Entity': 'Country', 'Total | 00002901 || Food available for consumption | 0674pc || grams of protein per day per capita': 'Daily Protein Supply'}, inplace=True)\n",
    "animal_plant_protein.rename(columns={\n",
    "    'Entity': 'Country',\n",
    "    'Animal Products | 00002941 || Food available for consumption | 0674pc || grams of protein per day per capita': 'Animal Protein Supply',\n",
    "    'Vegetal Products | 00002903 || Food available for consumption | 0674pc || grams of protein per day per capita': 'Vegetal Protein Supply'\n",
    "}, inplace=True)\n",
    "meat_consumption.rename(columns={'Entity': 'Country', 'Meat, total | 00002943 || Food available for consumption | 0645pc || kilograms per year per capita': 'Daily Meat Consumption'}, inplace=True)\n",
    "\n",
    "# Ensure the 'Year' column is consistently named\n",
    "caloric_supply.rename(columns={'Year': 'Year'}, inplace=True)\n",
    "protein_supply.rename(columns={'Year': 'Year'}, inplace=True)\n",
    "animal_plant_protein.rename(columns={'Year': 'Year'}, inplace=True)\n",
    "meat_consumption.rename(columns={'Year': 'Year'}, inplace=True)\n",
    "\n",
    "# Convert 'Daily Meat Consumption' from kg/year to g/day for consistency\n",
    "meat_consumption['Daily Meat Consumption'] = meat_consumption['Daily Meat Consumption'] * 1000 / 365\n",
    "\n",
    "# Rename columns in diabetes_data\n",
    "diabetes_data.rename(columns={'Percent Population with Diabetes': 'Diabetes Prevalence'}, inplace=True)\n",
    "\n",
    "# Merge the dataframes on 'Country' and 'Year'\n",
    "merged_df = caloric_supply.merge(protein_supply, on=['Country', 'Year'], how='outer')\n",
    "merged_df = merged_df.merge(animal_plant_protein, on=['Country', 'Year'], how='outer')\n",
    "merged_df = merged_df.merge(meat_consumption[['Country', 'Year', 'Daily Meat Consumption']], on=['Country', 'Year'], how='outer')\n",
    "merged_df = merged_df.merge(diabetes_data[['Country', 'Year', 'Diabetes Prevalence']], on=['Country', 'Year'], how='outer')\n",
    "\n",
    "# Select only the required columns\n",
    "final_df = merged_df[[\n",
    "    'Country', \n",
    "    'Year', \n",
    "    'Daily Caloric Supply', \n",
    "    'Daily Protein Supply', \n",
    "    'Animal Protein Supply', \n",
    "    'Vegetal Protein Supply', \n",
    "    'Daily Meat Consumption', \n",
    "    'Diabetes Prevalence'\n",
    "]]\n",
    "\n",
    "# Drop rows with any missing data\n",
    "final_df.dropna(inplace=True)\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "final_df.to_csv('Resources/final_data.csv', index=False)\n",
    "\n",
    "print(\"Final DataFrame columns:\", final_df.columns)\n",
    "print(\"Number of rows in Final DataFrame:\", len(final_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80ca270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Resources/final_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Select only the relevant features\n",
    "features = ['Daily Caloric Supply', 'Animal Protein Supply', 'Vegetal Protein Supply']\n",
    "target = 'Diabetes Prevalence'\n",
    "\n",
    "# Preprocess the data\n",
    "data = data.dropna()  # Remove missing values for simplicity\n",
    "\n",
    "# Define the features (X) and target (y)\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(80, input_dim=X_train_scaled.shape[1], activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    BatchNormalization(),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * np.exp(-0.1)\n",
    "\n",
    "callback = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_data=(X_test_scaled, y_test), callbacks=[callback])\n",
    "\n",
    "# Save the model\n",
    "model.save('Resources/diabetes_prevalence_model.h5')\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_scaled).flatten()\n",
    "\n",
    "# Evaluate the model on test data\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = np.mean(np.abs(y_test - y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Test MSE: {mse:.4f}\")\n",
    "print(f\"Test MAE: {mae:.4f}\")\n",
    "print(f\"Test R²: {r2:.4f}\")\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
